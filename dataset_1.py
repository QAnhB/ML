# -*- coding: utf-8 -*-
"""dataset_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H4wDyhQVP_mAijzxlzEoXLzcZDfr46XB

https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Coimbra?fbclid=IwAR0tC3sJnM-nXJDL04m2ATZc29Rd7wM--NrTguY8Mw67jQCEEtkH_e0CIAU

- samples: 64+52 = 116 samples
- 2 classes: healthy controls, patients
- 10 features: age, BMI, glusose, insulin, HOMA, leptin, adiponectin, resistin, MCP-1
"""

import pandas as pd
from sklearn.decomposition import PCA
from sklearn import preprocessing
import matplotlib.pyplot as plt
import numpy as np

df = pd.read_csv('D:\Downloads\ML\dataR2.csv')

# first 5 row 
df.head()

# drop the classification column 
x = df.drop(columns='Classification',axis=1)
print(x)

x.shape

"""1. Study the dataset
- Quantitative: Quantitative Attributes:
Age (years)
BMI (kg/m2)
Glucose (mg/dL)
Insulin (µU/mL)
HOMA
Leptin (ng/mL)
Adiponectin (µg/mL)
Resistin (ng/mL)
MCP-1(pg/dL)

- Discrete: age, glucose
- Continuous: BMI, Isnulin, HOMA, Leptin, Adionectin, Resistin, MCP-1 
"""

# mean
x.mean()

# variance 
x.var()

# covariance
x.cov()

# correlation
x_corr = x.corr()
x_corr

x_corr.describe()

upper_tri = x_corr.where(np.triu(np.ones(x_corr.shape),k=1).astype(bool))
print(upper_tri)

drop_null = upper_tri.unstack().dropna()
sorted_matrix = drop_null.sort_values()
print(sorted_matrix)

"""the most corelated couple of feautures of each dataset: HOMA, Insulin 
(highest correlation)

2. PCA
"""

# Importing standardscalar module 
from sklearn.preprocessing import StandardScaler
  
scalar = StandardScaler()
  
# fitting
scalar.fit(x)
scaled_data = scalar.transform(x)
scaled_data

# Importing PCA
from sklearn.decomposition import PCA
  
# Let's say, components = 2
pca = PCA(n_components = 2)
pca.fit(scaled_data)
x_pca = pca.transform(scaled_data)
  
x_pca.shape

# provide with the amount of information or variance each principal component holds after projecting the data to a lower dimensional subspace
pca.explained_variance_ratio_

# scree plot: how many principal components show into the final plot

# calculate the percentage of variation that each principal component accounts for
per_var = np.round(pca.explained_variance_ratio_*100, decimals = 1)

# create labels for the scree plot
labels = ['PC' + str(x) for x in range(1,len(per_var)+1)]

# create bar plot
plt.bar(x=range(1,len(per_var)+1), height = per_var, tick_label=labels)
plt.ylabel('Percentage of explained variance')
plt.xlabel('Principal component')
plt.title('Scree Plot')
plt.show()
# almost all of the variation is along the fist PC 
# 2D grap, using PC1 & PC2 should do a good job representing the original data

"""- PC1 holds 33.9% of the information
- PC2 holds 16.9% of the information
- in the other words, while projecting ten-demensional data to two-dimensional data, 50.8% informatin was hold, 49.2% was lost
"""

# create a DataFrame that will have the principal component values for all 116 samples
pca_Df = pd.DataFrame(data = x_pca, columns = ['PC1', 'PC2'])
pca_Df.head()

final_df=pd.concat([pca_Df,df[['Classification']]],axis=1)
final_df

final_df['Classification'].replace(1, 'Healthy controls',inplace=True)
final_df['Classification'].replace(2, 'Patients',inplace=True)

final_df.tail()

# Visualize data distribution in 2D
plt.figure()
plt.figure(figsize=(10,10))
plt.xticks(fontsize=12)
plt.yticks(fontsize=14)
plt.xlabel('PC1', fontsize=20)
plt.ylabel('PC2',fontsize=20)
plt.title("PCA plot",fontsize=20)
targets = ['Healthy controls', 'Patients']
colors = ['r', 'b']
for target, color in zip(targets,colors):
    indicesToKeep = final_df['Classification'] == target
    plt.scatter(final_df.loc[indicesToKeep, 'PC1'], final_df.loc[indicesToKeep, 'PC2'], c = color, s = 50)
    #plt.scatter(pca_df.PC1, pca_df.PC2, c=color, s=50)

plt.legend(targets,prop={'size': 15})
plt.show()

"""Increase the number of principal components used"""

# 3 components
pca3 = PCA(n_components = 3)
pca3.fit(scaled_data)
x_pca3 = pca3.transform(scaled_data)
x_pca3.shape

pca3.explained_variance_ratio_

# scree plot
per_var = np.round(pca3.explained_variance_ratio_*100, decimals = 1)

# create labels for the scree plot
labels = ['PC' + str(x) for x in range(1,len(per_var)+1)]

# create bar plot
plt.bar(x=range(1,len(per_var)+1), height = per_var, tick_label=labels)
plt.ylabel('Percentage of explained variance')
plt.xlabel('Principal component')
plt.title('Scree Plot')
plt.show()

pca_ = PCA(0.85) # 85% retained variance
pca_.fit(scaled_data)
x_pca_ = pca_.transform(scaled_data)
  
x_pca_.shape

pca_.explained_variance_ratio_

# scree plot
per_var = np.round(pca_.explained_variance_ratio_*100, decimals = 1)

# create labels for the scree plot
labels = ['PC' + str(x) for x in range(1,len(per_var)+1)]

# create bar plot
plt.bar(x=range(1,len(per_var)+1), height = per_var, tick_label=labels)
plt.ylabel('Percentage of explained variance')
plt.xlabel('Principal component')
plt.title('Scree Plot')
plt.show()

"""Test"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(scaled_data, df['Classification'], test_size=0.2, random_state=30)

from sklearn.linear_model import LogisticRegression

model = LogisticRegression();
model.fit(X_train, y_train)
model.score(X_test, y_test)

X_train_pca, X_test_pca, y_train, y_test = train_test_split(x_pca, df['Classification'], test_size=0.2, random_state=30)

model = LogisticRegression(max_iter=1000);
model.fit(X_train_pca, y_train)
model.score(X_test_pca, y_test)
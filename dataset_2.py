# -*- coding: utf-8 -*-
"""dataset_2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_KCGF5pDfs4QM1yM_EWchtW3Sa1CsUtR

https://archive.ics.uci.edu/ml/datasets/Raisin+Dataset

Kecimen and Besni raisin varieties
- 900 samples: 450 from both varieties
- 7 features
- Class: Kecimen and Besni raisin

1. Study the dataset
- Quantitative: Area, Perimeter, MajorAxisLength, MinorAxisLength, Eccentricity, ConvexArea, Extent

- Discrete: Area, ConvexArea
- Continuous: the others
"""

import pandas as pd
from sklearn.decomposition import PCA
from sklearn import preprocessing
import matplotlib.pyplot as plt
import numpy as np

df = pd.read_csv('D:\Downloads\ML\Raisin_Dataset.csv')

# first 5 row 
print("dataset head\n",df.head())

# drop the classification column 
print("\nDrop classfication column")
x = df.drop(columns='Class',axis=1)
print(x)

print("\nX shape:",x.shape)

# mean
print("Mean:\n",x.mean())

# variance 
print("Variance:\n",x.var())

# covariance
print("Covariance:\n",x.cov())

# correlation
x_corr = x.corr()
print("Correlation:\n",x_corr)

print("Describe:\n",x_corr.describe())

# take upper triangle
# avoid duplicate data 
upper_tri = x_corr.where(np.triu(np.ones(x_corr.shape),k=1).astype(bool))
print("Upper triangle: ",upper_tri)

# arrange correlation
drop_null = upper_tri.unstack().dropna()
sorted_matrix = drop_null.sort_values()
print("Sorted correlation: \n",sorted_matrix)

"""the most corelated couple of feautures of each dataset: ConvexArea, Are (highest correlation)

2. PCA
"""

# Importing standardscalar module 
from sklearn.preprocessing import StandardScaler
  
scalar = StandardScaler()
  
# fitting
scalar.fit(x)
scaled_data = scalar.transform(x)
scaled_data

# Importing PCA
from sklearn.decomposition import PCA
  
# Let's say, components = 2
pca = PCA(n_components = 2)
pca.fit(scaled_data)
x_pca = pca.transform(scaled_data)
  
x_pca.shape

# provide with the amount of information or variance each principal component holds after projecting the data to a lower dimensional subspace
pca.explained_variance_ratio_

# scree plot: how many principal components show into the final plot

# calculate the percentage of variation that each principal component accounts for
per_var = np.round(pca.explained_variance_ratio_*100, decimals = 1)

# create labels for the scree plot
labels = ['PC' + str(x) for x in range(1,len(per_var)+1)]

# create bar plot
plt.bar(x=range(1,len(per_var)+1), height = per_var, tick_label=labels)
plt.ylabel('Percentage of explained variance')
plt.xlabel('Principal component')
plt.title('Scree Plot')
plt.show()
# almost all of the variation is along the fist PC 
# 2D grap, using PC1 & PC2 should do a good job representing the original data

"""- PC1 holds 69% of the information
- PC2 holds 20.7% of the information
- in the other words, while projecting ten-demensional data to two-dimensional data, 89.7% informatin was hold, 10.3% was lost
"""

# create a DataFrame that will have the principal component values for all 116 samples
pca_Df = pd.DataFrame(data = x_pca, columns = ['PC1', 'PC2'])
pca_Df.head()

final_df=pd.concat([pca_Df,df[['Class']]],axis=1)
final_df

# Visualize data distribution in 2D
plt.figure()
plt.figure(figsize=(10,10))
plt.xticks(fontsize=12)
plt.yticks(fontsize=14)
plt.xlabel('PC1', fontsize=20)
plt.ylabel('PC2',fontsize=20)
plt.title("PCA plot",fontsize=20)
targets = ['Kecimen', 'Besni']
colors = ['r', 'b']
for target, color in zip(targets,colors):
    indicesToKeep = final_df['Class'] == target
    plt.scatter(final_df.loc[indicesToKeep, 'PC1'], final_df.loc[indicesToKeep, 'PC2'], c = color, s = 50)
    #plt.scatter(pca_df.PC1, pca_df.PC2, c=color, s=50)

plt.legend(targets,prop={'size': 15})
plt.show()

"""Looks great, isn’t it? The two classes are well separated with the first 2 principal components as new features. As good as it seems like even a linear classifier could do very well to identify a class from the test set. On a separate post, I have discussed how to apply a pipeline consisting of PCA and Support Vector Classifier to and draw the decision function for this same data-set. One important feature is how the malignant class is spread out compared to benign and take a look back to those histogram plots. Can you find some similarity?

These principal components are calculated only from features and no information from classes are considered. So PCA is unsupervised method and it’s difficult to interpret the two axes as they are some complex mixture of the original features. We can make a heat-plot to see how the features mixed up to create the components

- observe that the two classes benign and malignant,
- when projected to a two-dimensional space, can be linearly separable up to some extent
- there observations can be that the benign class is spread out as compared to the malignant class

https://www.datacamp.com/tutorial/principal-component-analysis-in-python

Increase the number of principal components used
"""

# 3 components
pca_ = PCA()
pca_.fit(scaled_data)
x_pca_ = pca_.transform(scaled_data)
x_pca_.shape

pca_.explained_variance_ratio_

# scree plot
per_var = np.round(pca_.explained_variance_ratio_*100, decimals = 1)

# create labels for the scree plot
labels = ['PC' + str(x) for x in range(1,len(per_var)+1)]

# create bar plot
plt.bar(x=range(1,len(per_var)+1), height = per_var, tick_label=labels)
plt.ylabel('Percentage of explained variance')
plt.xlabel('Principal component')
plt.title('Scree Plot')
plt.show()

y = df['Class']

y.replace('Kecimen', 0,inplace=True)
y.replace('Besni', 1,inplace=True)

y

"""Test"""

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(scaled_data, y, test_size=0.2, random_state=30)

from sklearn.linear_model import LogisticRegression

model = LogisticRegression();
model.fit(X_train, y_train)
print("Model score before pca: ",model.score(X_test, y_test))

X_train_pca, X_test_pca, y_train, y_test = train_test_split(x_pca, y, test_size=0.2, random_state=30)

model = LogisticRegression(max_iter=1000);
model.fit(X_train_pca, y_train)
print("Model score after pca: ",model.score(X_test_pca, y_test))